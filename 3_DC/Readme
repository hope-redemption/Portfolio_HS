Research on smart car movement and precise positioning control based on continuous gesture perception                                                                                         Nov 2021 – Sep 2022
Supervised by Dr. Zaiyue Yang in Intelligent System and Decision Making Lab, SUSTech         

Project Description:
Target: 
Aimed to build a natural human-computer interaction system for the command and scheduling of mobile robots.
Content:
Awarded a grant of $2000 as a provincial college students’ entrepreneurship project
Realized intelligent car movement and precise positioning control tasks such as target pointing, target confirmation, and position fine-tuning through the interaction of natural, reasonable, and fluent hand gestures.
Proposed an adaptive motion control strategy for the mobile robot in unstructured scenarios
Proposed an EKF optimization algorithm based on angle observation to improve the ability of intelligent car movement and precise positioning
Proposed a mobile robot command and dispatch interactive system based on hand motion capture. As the commander, the human gives different task-scheduling signals through gestures according to the current state of the car. The signals generated by the hand output the identified information to the intelligent car through the human intention perception module. The car implements different motion control strategies according to the current task and autonomous environment perception and controls the execution of the corresponding movement or specific tasks (such as the initial alignment of the human and the car, Correct vehicle position, correct human position, lock auxiliary markers, determine the target).
